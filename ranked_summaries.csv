Rank,Title,Abstract,PDF Link,First Author Name,First Author H-index,Second Author Name,Second Author H-index,Predicted_Score,Publish Date,Topic,Summary
1,"=HYPERLINK(""http://arxiv.org/abs/2501.15957v1"", ""Inverse Reinforcement Learning via Convex Optimization"")","We consider the inverse reinforcement learning (IRL) problem, where an
unknown reward function of some Markov decision process is estimated based on
observed expert demonstrations. In most existing approaches, IRL is formulated
and solved as a nonconvex optimization problem, posing challenges in scenarios
where robustness and reproducibility are critical. We discuss a convex
formulation of the IRL...",http://arxiv.org/pdf/2501.15957v1.pdf,Hao Zhu,91.0,Yuan Zhang,136.0,137.33409,2025-01-27T11:03:18Z,Reinforcement Learning,"This text discusses Inverse Reinforcement Learning (IRL), which estimates an unknown reward function of a Markov decision process based on expert demonstrations. The authors propose a convex formulation of the IRL problem (CIRL) to address challenges with nonconvex optimization. They also extend the CIRL problem to handle scenarios where the expert policy is not optimally consistent. The methodology involves using convex optimization techniques and hyperparameter auto-selection. The real-world impact includes applications in autonomous driving, robotics, psychology, and neuroscience research. The CIRL approach simplifies the application of IRL for users without expertise in convex optimization, ensuring reproducibility and robustness in results. The authors also provide open-source code for implementing the CIRL method. Overall, this work enhances the usability and effectiveness of IRL in various fields."
2,"=HYPERLINK(""http://arxiv.org/abs/2501.12620v1"", ""Adaptive Data Exploitation in Deep Reinforcement Learning"")","We introduce ADEPT: Adaptive Data ExPloiTation, a simple yet powerful
framework to enhance the **data efficiency** and **generalization** in deep
reinforcement learning (RL). Specifically, ADEPT adaptively manages the use of
sampled data across different learning stages via multi-armed bandit (MAB)
algorithms, optimizing data utilization while mitigating overfitting. Moreover,
ADEPT can significan...",http://arxiv.org/pdf/2501.12620v1.pdf,Mingqi Yuan,5.0,Bo Li,95.0,31.358206,2025-01-22T04:01:17Z,Reinforcement Learning,"ADEPT is a framework that enhances data efficiency and generalization in deep reinforcement learning (RL) by adaptively managing the use of sampled data using multi-armed bandit (MAB) algorithms. It reduces computational overhead and accelerates RL algorithms, testing successfully on benchmarks like Procgen, MiniGrid, and PyBullet. ADEPT improves data efficiency and generalization, with minimal computational costs. It offers a practical solution for data-efficient RL. The framework is adaptable and can be easily integrated into various RL algorithms, improving performance and reducing computational overhead."
3,"=HYPERLINK(""http://arxiv.org/abs/2501.15217v1"", ""Predictive Lagrangian Optimization for Constrained Reinforcement
  Learning"")","Constrained optimization is popularly seen in reinforcement learning for
addressing complex control tasks. From the perspective of dynamic system,
iteratively solving a constrained optimization problem can be framed as the
temporal evolution of a feedback control system. Classical constrained
optimization methods, such as penalty and Lagrangian approaches, inherently use
proportional and integral ...",http://arxiv.org/pdf/2501.15217v1.pdf,Tianqi Zhang,52.0,Puzhen Yuan,1.0,24.6273,2025-01-25T13:39:45Z,Other,"This paper introduces a new algorithm called predictive Lagrangian optimization (PLO) that leverages model predictive control to improve constrained reinforcement learning. The method establishes a framework connecting constrained optimization with feedback control systems, allowing for the development of more effective algorithms. Numerical experiments show that PLO outperforms existing methods in terms of safety constraints and average rewards. This research has real-world impact on complex control tasks by improving safety and performance in reinforcement learning applications."
4,"=HYPERLINK(""http://arxiv.org/abs/2501.15893v1"", ""Benchmarking Quantum Reinforcement Learning"")","Benchmarking and establishing proper statistical validation metrics for
reinforcement learning (RL) remain ongoing challenges, where no consensus has
been established yet. The emergence of quantum computing and its potential
applications in quantum reinforcement learning (QRL) further complicate
benchmarking efforts. To enable valid performance comparisons and to streamline
current research in thi...",http://arxiv.org/pdf/2501.15893v1.pdf,Nico Meyer,47.0,Christian Ufrecht,27.0,22.686436,2025-01-27T09:40:18Z,Reinforcement Learning,"Benchmarking Quantum Reinforcement Learning aims to address ongoing challenges in benchmarking and validating statistical metrics for reinforcement learning, especially in the context of quantum reinforcement learning (QRL). The study proposes a novel benchmarking methodology based on a statistical estimator for sample complexity and statistical outperformance. Experiments conducted on a novel benchmarking environment with flexible complexity reveal nuanced findings, casting doubt on previous claims regarding the superiority of QRL. The proposed methodology can help streamline research in this area and enable valid performance comparisons. By addressing the problem of benchmarking and validating QRL algorithms, this study has real-world implications for the development and evaluation of quantum machine learning solutions."
5,"=HYPERLINK(""http://arxiv.org/abs/2501.09080v1"", ""Average-Reward Reinforcement Learning with Entropy Regularization"")","The average-reward formulation of reinforcement learning (RL) has drawn
increased interest in recent years due to its ability to solve
temporally-extended problems without discounting. Independently, RL algorithms
have benefited from entropy-regularization: an approach used to make the
optimal policy stochastic, thereby more robust to noise. Despite the distinct
benefits of the two approaches, the...",http://arxiv.org/pdf/2501.09080v1.pdf,Jacob Adamczyk,4.0,Volodymyr Makarenko,90.0,21.379808,2025-01-15T19:00:46Z,Reinforcement Learning,"The text explores the combination of average-reward reinforcement learning (RL) with entropy regularization, addressing the limitations of discounting in RL algorithms. By developing algorithms for solving entropy-regularized average-reward RL problems with function approximation, the authors experimentally validate their method, showing improved performance in continuous control environments. The main contributions include extending the policy improvement theorem to the average-reward case, providing novel algorithms for average-reward deep RL, and experimentally validating their approach. This work has the potential to advance the field of average-reward RL and improve real-world applications of RL algorithms."
6,"=HYPERLINK(""http://arxiv.org/abs/2501.04481v1"", ""Safe Reinforcement Learning with Minimal Supervision"")","Reinforcement learning (RL) in the real world necessitates the development of
procedures that enable agents to explore without causing harm to themselves or
others. The most successful solutions to the problem of safe RL leverage
offline data to learn a safe-set, enabling safe online exploration. However,
this approach to safe-learning is often constrained by the demonstrations that
are available ...",http://arxiv.org/pdf/2501.04481v1.pdf,Alexander Quessy,1.0,Thomas Richardson,65.0,16.799212,2025-01-08T13:04:08Z,Reinforcement Learning,"Reinforcement learning (RL) in real-world scenarios must ensure agents explore safely. Current solutions rely on offline data to train safe sets for online exploration, but often lack sufficient demonstrations. This research investigates the impact of quantity and quality of data on safe RL policies, proposing optimistic forgetting to address limited data scenarios. An unsupervised RL-based data collection approach is suggested for complex tasks, emphasizing the balance between diversity and optimality for safe exploration. The study highlights the importance of providing enough demonstrations for agents to learn optimal safe RL policies online. The proposed approach has real-world implications for developing safe RL policies without the need for hand-designed controllers or user demonstrations, making it more scalable and efficient for complex tasks and environments. By addressing the challenges of safe RL with minimal supervision, this research contributes to advancing the field of RL in real-world applications."
7,"=HYPERLINK(""http://arxiv.org/abs/2501.11818v1"", ""Group-Agent Reinforcement Learning with Heterogeneous Agents"")","Group-agent reinforcement learning (GARL) is a newly arising learning
scenario, where multiple reinforcement learning agents study together in a
group, sharing knowledge in an asynchronous fashion. The goal is to improve the
learning performance of each individual agent. Under a more general
heterogeneous setting where different agents learn using different algorithms,
we advance GARL by designing...",http://arxiv.org/pdf/2501.11818v1.pdf,Kaiyue Wu,7.0,Xiao-Jun Zeng,46.0,16.59704,2025-01-21T01:56:33Z,Reinforcement Learning,"Group-Agent Reinforcement Learning (GARL) involves multiple reinforcement learning agents studying together in a group to improve individual learning. In a heterogeneous setting, where agents use different algorithms, Group-Agent Reinforcement Learning (HGARL) was developed. Agents share policy and value parameters and accumulated reward scores to improve learning. Three action selection rules were introduced - Probability Addition, Probability Multiplication, and Reward-Value-Likelihood Combination. Extensive experiments on Atari 2600 games showed a significant performance improvement with 96% achieving a learning speed-up and 72% learning over 100 times faster. The Combo Rule was found to be the most effective in selecting actions. The methodology allows agents to adopt better models from others, leading to faster learning and improved reward scores. The real-world impact includes enhancing performance in applications like video game playing and autonomous driving. This innovative approach advances the field of reinforcement learning by facilitating effective group learning for heterogeneous agents."
8,"=HYPERLINK(""http://arxiv.org/abs/2501.16142v1"", ""Towards General-Purpose Model-Free Reinforcement Learning"")","Reinforcement learning (RL) promises a framework for near-universal
problem-solving. In practice however, RL algorithms are often tailored to
specific benchmarks, relying on carefully tuned hyperparameters and algorithmic
choices. Recently, powerful model-based RL methods have shown impressive
general results across benchmarks but come at the cost of increased complexity
and slow run times, limiti...",http://arxiv.org/pdf/2501.16142v1.pdf,Scott Fujimoto,13.0,Pierluca D'Oro,9.0,16.094969,2025-01-27T15:36:37Z,Reinforcement Learning,"This text introduces a new algorithm, MR.Q, that aims to create a general-purpose model-free deep reinforcement learning approach. The goal is to address the limitations of current RL algorithms, which are often tailored to specific benchmarks and require extensive tuning. MR.Q leverages model-based representations to linearize the value function and achieve competitive performance across various domains and benchmarks. This novel algorithm shows promising results, outperforming both domain-specific and general baselines while using a single set of hyperparameters. By simplifying the complexity and improving efficiency, MR.Q paves the way for more practical and widely applicable RL algorithms in the real world."
9,"=HYPERLINK(""http://arxiv.org/abs/2501.11463v1"", ""Curiosity-Driven Reinforcement Learning from Human Feedback"")","Reinforcement learning from human feedback (RLHF) has proven effective in
aligning large language models (LLMs) with human preferences, but often at the
cost of reduced output diversity. This trade-off between diversity and
alignment quality remains a significant challenge. Drawing inspiration from
curiosity-driven exploration in reinforcement learning, we introduce
curiosity-driven RLHF (CD-RLHF)...",http://arxiv.org/pdf/2501.11463v1.pdf,Haoran Sun,11.0,Yekun Chai,10.0,15.756671,2025-01-20T12:51:40Z,Reinforcement Learning,"The text introduces a new framework called Curiosity-Driven Reinforcement Learning from Human Feedback (CD-RLHF) to address the trade-off between output diversity and alignment quality in large language models (LLMs). By incorporating intrinsic rewards for novel states, alongside traditional rewards, CD-RLHF optimizes both diversity and alignment. Experiments on tasks like text summarization and instruction following show that CD-RLHF achieves higher output diversity while maintaining alignment quality comparable to standard RLHF. This framework has real-world impact in improving the effectiveness of LLMs in creative and open-ended tasks like story generation and data synthesis. Overall, CD-RLHF is an innovative approach to balancing the trade-off in reinforcement learning from human feedback."
10,"=HYPERLINK(""http://arxiv.org/abs/2501.12599v1"", ""Kimi k1.5: Scaling Reinforcement Learning with LLMs"")","Language model pretraining with next token prediction has proved effective
for scaling compute but is limited to the amount of available training data.
Scaling reinforcement learning (RL) unlocks a new axis for the continued
improvement of artificial intelligence, with the promise that large language
models (LLMs) can scale their training data by learning to explore with
rewards. However, prior pu...",http://arxiv.org/pdf/2501.12599v1.pdf, Kimi Team,0.0,Angang Du,11.0,15.142981,2025-01-22T02:48:14Z,Reinforcement Learning,"The text discusses the training of a new AI model called Kimi k1.5, which combines reinforcement learning with large language models to improve reasoning performance. Key findings include state-of-the-art results on various benchmarks and modalities, outperforming existing models. The methodology includes long context scaling, improved policy optimization, and effective training techniques. The model's real-world impact lies in its ability to enhance AI reasoning capabilities and generalization to unseen problems, without relying on complex techniques. These advancements pave the way for improved AI performance in various domains such as coding and general reasoning tasks."
